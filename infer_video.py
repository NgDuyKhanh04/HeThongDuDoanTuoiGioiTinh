
# D·ª± ƒëo√°n TU·ªîI + GI·ªöI T√çNH tr√™n VIDEO (kh√¥ng d√πng webcam/URL)
# ==============================

import os, sys, cv2, numpy as np
from collections import OrderedDict
from tensorflow.keras.models import load_model

# ---- C·∫•u h√¨nh ----
DUONG_DAN_MO_HINH = "age_gender_model_1.keras"
KICH_THUOC_ANH = 64
TUOI_TOI_DA = 100.0
MIN_FACE_SIZE = 80  # px

# Nh√£n hi·ªÉn th·ªã
TEXT_MALE = "Nam"
TEXT_FEMALE = "Nu"
TEXT_UNKNOWN = "Khong r√µ"

# Ng∆∞·ª°ng quy·∫øt ƒë·ªãnh
NGUONG_GIOITINH = 0.50
BIEN_DO_GIOITINH = 0.10
TIN_CAY_NHOM = 0.40

NHOM_TUOI_TEN  = ["0-12","13-17","18-24","25-34","35-44","45-54","55-64","65+"]

# L√†m m∆∞·ª£t EMA theo ID khu√¥n m·∫∑t
ALPHA_TUOI = 0.6
ALPHA_GIOITINH = 0.6
ALPHA_NHOM = 0.6

# ƒê·ªãnh d·∫°ng video h·ª£p l·ªá
EXTS_VIDEO = {".mp4", ".avi", ".mov", ".mkv", ".webm", ".m4v"}

# ---------------- utils ----------------
def _clean(s: str) -> str:
    if s is None: return ""
    return s.strip().strip('"').strip("'").strip()

def _resolve_video_path(user_in: str) -> str:

    p = _clean(user_in)
    if not p:
        return p

    # N·∫øu ng∆∞·ªùi d√πng l·ª° d√°n c·∫£ "python infer_video.py xxx", l·∫•y token cu·ªëi
    parts = p.split()
    cand = _clean(parts[-1])

    # N·∫øu ƒë√£ t·ªìn t·∫°i -> d√πng lu√¥n
    if os.path.exists(cand):
        return cand

    # N·∫øu ch·ªâ l√† t√™n file -> gh√©p v·ªõi th∆∞ m·ª•c ch∆∞∆°ng tr√¨nh (Age_Gender)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    if os.path.dirname(cand) == "":
        trial = os.path.join(script_dir, cand)
        if os.path.exists(trial):
            return trial

    # Kh√¥ng t√¨m th·∫•y -> tr·∫£ v·ªÅ nguy√™n b·∫£n (ƒë·ªÉ b√°o l·ªói)
    return cand

def _is_video_ext(path: str) -> bool:
    return os.path.splitext(path)[1].lower() in EXTS_VIDEO

def _preprocess(face_bgr):
    im = cv2.resize(face_bgr, (KICH_THUOC_ANH, KICH_THUOC_ANH))
    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
    im = im.astype("float32")/255.0
    return np.expand_dims(im, 0)

def _annotate(frame, x,y,w,h, text):
    cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)
    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
    cv2.rectangle(frame, (x, y-8-th-6), (x+tw+6, y-4), (0,0,0), -1)
    cv2.putText(frame, text, (x+3, y-8), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)

def _ema(old, new, a):
    if old is None: return new
    return old*(1-a) + new*a

def _assign_ids(faces, tracks, nguong=110):
    centers = [(x+w//2, y+h//2) for (x,y,w,h) in faces]
    used = set()
    new_tracks = OrderedDict()
    for tid, st in tracks.items():
        if not centers: continue
        d = [abs(cx-st['cx']) + abs(cy-st['cy']) for (cx,cy) in centers]
        j = int(np.argmin(d))
        if j in used or d[j] > nguong:
            continue
        cx, cy = centers[j]
        new_tracks[tid] = {'cx':cx,'cy':cy,'bbox':faces[j],
                           'p_nhom':st.get('p_nhom'),
                           'tuoi':st.get('tuoi'),
                           'gioi':st.get('gioi')}
        used.add(j)
    next_id = (max(tracks.keys())+1) if len(tracks) else 1
    for k,(cx,cy) in enumerate(centers):
        if k in used: continue
        new_tracks[next_id] = {'cx':cx,'cy':cy,'bbox':faces[k],
                               'p_nhom':None,'tuoi':None,'gioi':None}
        next_id += 1
    return new_tracks

# ---------------- main ----------------
def main():
    # 1) L·∫•y ƒë∆∞·ªùng d·∫´n video
    if len(sys.argv) >= 2:
        raw = " ".join(sys.argv[1:])
    else:
        print(" python infer_video.py <ten_file_or_duong_dan_video>")
        try:
            raw = input("Nh·∫≠p T√äN FILE  ho·∫∑c ƒë∆∞·ªùng d·∫´n video: ")
        except EOFError:
            raw = ""

    video_path = _resolve_video_path(raw)

    if not os.path.exists(video_path):
        print(" Kh√¥ng th·∫•y file:", video_path); return
    if not _is_video_ext(video_path):
        print(" File kh√¥ng ph·∫£i video. D√πng infer_image.py cho ·∫£nh."); return

    # 2) Load model
    try:
        model = load_model(DUONG_DAN_MO_HINH, compile=False)
    except Exception as e:
        print(" L·ªói load m√¥ h√¨nh:", e); return

    # 3) M·ªü video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(" Kh√¥ng m·ªü ƒë∆∞·ª£c video:", video_path); return

    # 4) VideoWriter (ghi c√πng th∆∞ m·ª•c v·ªõi ngu·ªìn, th√™m _out.mp4)
    base, _ = os.path.splitext(video_path)
    out_path = base + "_out.mp4"

    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 640
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 480
    fps = cap.get(cv2.CAP_PROP_FPS)
    if not fps or fps <= 1: fps = 30
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(out_path, fourcc, fps, (w, h))
    print(f"üíæ Ghi ra: {out_path} | {w}x{h} @ {fps:.1f}fps")
    print("Nh·∫•n 'q' ƒë·ªÉ d·ª´ng s·ªõm.")

    # 5) D√≤ m·∫∑t + d·ª± ƒëo√°n
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
    tracks = OrderedDict()

    while True:
        ok, frame = cap.read()
        if not ok:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces_np = face_cascade.detectMultiScale(gray, 1.2, 5, minSize=(MIN_FACE_SIZE, MIN_FACE_SIZE))
        faces = list(faces_np) if faces_np is not None else []
        tracks = _assign_ids(faces, tracks, nguong=110)

        for tid, st in tracks.items():
            x,y,wf,hf = st['bbox']
            if wf < MIN_FACE_SIZE or hf < MIN_FACE_SIZE: 
                continue
            roi = frame[y:y+hf, x:x+wf]
            if roi.size == 0: 
                continue

            xin = _preprocess(roi)
            try:
                ypred = model.predict(xin, verbose=0)
            except Exception as e:
                print(" L·ªói d·ª± ƒëo√°n:", e); 
                continue

            if not isinstance(ypred, (list,tuple)) or len(ypred)!=3:
                continue

            p_nhom, p_tuoi, p_gioi = ypred

            # Nh√≥m tu·ªïi
            vec = p_nhom[0]
            st['p_nhom'] = vec if st['p_nhom'] is None else _ema(st['p_nhom'], vec, ALPHA_NHOM)
            idx = int(np.argmax(st['p_nhom']))
            conf = float(st['p_nhom'][idx])
            scope = NHOM_TUOI_TEN[idx] if conf >= TIN_CAY_NHOM else f"{NHOM_TUOI_TEN[idx]}?"

            # Tu·ªïi
            age = float(p_tuoi[0][0]) * TUOI_TOI_DA
            st['tuoi'] = age if st['tuoi'] is None else _ema(st['tuoi'], age, ALPHA_TUOI)
            age_show = int(round(st['tuoi']))

            # Gi·ªõi t√≠nh
            prob = float(p_gioi[0][0])
            st['gioi'] = prob if st['gioi'] is None else _ema(st['gioi'], prob, ALPHA_GIOITINH)
            p = st['gioi']
            if abs(p - NGUONG_GIOITINH) <= BIEN_DO_GIOITINH:
                g = TEXT_UNKNOWN
            else:
                g = TEXT_MALE if p < NGUONG_GIOITINH else TEXT_FEMALE

            _annotate(frame, x,y,wf,hf, f"{g}, {scope} (~{age_show})")

        out.write(frame)
        cv2.imshow("Age & Gender - VIDEO", frame)
        if (cv2.waitKey(1) & 0xFF) == ord('q'):
            break

    cap.release(); out.release()
    cv2.destroyAllWindows()
    print(" Ho√†n t·∫•t. File ƒë√£ ghi:", out_path)

if __name__ == "__main__":
    main()
